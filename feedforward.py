# -*- coding: utf-8 -*-
"""FeedForward.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Hd5_DXbibykqOtZWsEo5Yicm8SHwgnzw
"""

import random
import glob
import subprocess
import os
from PIL import Image
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras import layers
from tensorflow.keras import backend as K
from tensorflow.keras.callbacks import Callback
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate
from tensorflow.keras.layers import PReLU, LeakyReLU
from tensorflow.keras.layers import BatchNormalization
import matplotlib.pyplot as plt
import wandb
from wandb.keras import WandbCallback

run = wandb.init(project='superres')
config = run.config

config.num_epochs = 100
config.batch_size = 1
config.input_height = 32
config.input_width = 32
config.output_height = 256
config.output_width = 256

val_dir = 'data/test'
train_dir = 'data/train'

# automatically get the data if it doesn't exist
if not os.path.exists("data"):
    print("Downloading flower dataset...")
    subprocess.check_output(
        "mkdir data && curl https://storage.googleapis.com/wandb/flower-enhance.tar.gz | tar xzf - -C data", shell=True)

config.steps_per_epoch = len(
    glob.glob(train_dir + "/*-in.jpg")) // config.batch_size
config.val_steps_per_epoch = len(
    glob.glob(val_dir + "/*-in.jpg")) // config.batch_size

def image_generator(batch_size, img_dir):
    """A generator that returns small images and large images.  DO NOT ALTER the validation set"""
    input_filenames = glob.glob(img_dir + "/*-in.jpg")
    counter = 0
    random.shuffle(input_filenames)
    while True:
        small_images = np.zeros(
            (batch_size, config.input_width, config.input_height, 3))
        large_images = np.zeros(
            (batch_size, config.output_width, config.output_height, 3))
        if counter+batch_size >= len(input_filenames):
            counter = 0
        for i in range(batch_size):
            img = input_filenames[counter + i]
            small_images[i] = np.array(Image.open(img)) / 255.0
            large_images[i] = np.array(
                Image.open(img.replace("-in.jpg", "-out.jpg"))) / 255.0
        yield (small_images, large_images)
        counter += batch_size


def perceptual_distance(y_true, y_pred):
    """Calculate perceptual distance, DO NOT ALTER"""
    y_true *= 255
    y_pred *= 255
    rmean = (y_true[:, :, :, 0] + y_pred[:, :, :, 0]) / 2
    r = y_true[:, :, :, 0] - y_pred[:, :, :, 0]
    g = y_true[:, :, :, 1] - y_pred[:, :, :, 1]
    b = y_true[:, :, :, 2] - y_pred[:, :, :, 2]

    return K.mean(K.sqrt((((512+rmean)*r*r)/256) + 4*g*g + (((767-rmean)*b*b)/256)))

val_generator = image_generator(config.batch_size, val_dir)
in_sample_images, out_sample_images = next(val_generator)

class ImageLogger_save(Callback):
    def on_epoch_end(self, epoch, logs):
        os.makedirs('ffn', exist_ok=True)
        titles = ['IN', 'GEN', 'ORG']
        preds = self.model.predict(in_sample_images)
        
        fig, axs = plt.subplots(1, 3)
        for j, image in enumerate([in_sample_images, preds, out_sample_images]):
            axs[j].imshow(image[0])
            axs[j].set_title(titles[j])
            axs[j].axis('off')
        fig.savefig("ffn/sample_%d.png" % epoch)
        plt.close()

class ImageLogger(Callback):
    def on_epoch_end(self, epoch, logs):
        preds = self.model.predict(in_sample_images)
        
        in_resized = []
        for arr in in_sample_images:
            # Simple upsampling
            in_resized.append(arr.repeat(8, axis=0).repeat(8, axis=1))
        wandb.log({
            "examples": [wandb.Image(np.concatenate([in_resized[i] * 255, o * 255, out_sample_images[i] * 255], axis=1)) for i, o in enumerate(preds)]
        }, commit=False)

class FFN():
    def __init__(self):
        self.img_rows = 256
        self.img_cols = 256
        self.channels = 3
        self.img_shape = (self.img_rows, self.img_cols, self.channels)
        self.img_lr_shape = (32, 32, 3)

        self.sample_generator = image_generator(2, train_dir)

        optimizer = Adam(0.0002, 0.5)

        # Build and compile the generator
        self.model = self.build_generator()
        self.model.compile(loss='mse', 
                           optimizer=optimizer, 
                           metrics=[perceptual_distance])
    
    def build_generator(self):

        def residual_block(layer_input, filters):
            #"""Residual block described in paper"""
            d = layers.Conv2D(filters, kernel_size=3, strides=1, padding='same')(layer_input)
            d = layers.Activation('relu')(d)
            d = layers.BatchNormalization(momentum=0.8)(d)
            d = layers.Conv2D(filters, kernel_size=3, strides=1, padding='same')(d)
            d = layers.BatchNormalization(momentum=0.8)(d)
            d = layers.Add()([d, layer_input])
            return d

        def deconv2d(layer_input):
            #"""Layers used during upsampling"""
            u = layers.UpSampling2D(size=2)(layer_input)
            u = layers.Conv2D(256, kernel_size=3, strides=1, padding='same')(u)
            u = layers.Activation('relu')(u)
            return u

        img_lr_shape = (config.input_width, config.input_height, 3)

        model = Sequential()
        img_lr = Input(shape=img_lr_shape) 

        c1 = layers.Conv2D(64, kernel_size=9, strides=1, padding='same')(img_lr * 2 - 1)
        c1 = layers.Activation('relu')(c1)

        # Local variables
        self.gf = 64
        self.n_residual_blocks = 16

        # Propogate through residual blocks
        r = residual_block(c1, self.gf)
        for _ in range(self.n_residual_blocks - 1):
            r = residual_block(r, self.gf)

        # Post-residual block
        c2 = layers.Conv2D(64, kernel_size=3, strides=1, padding='same')(r)
        c2 = layers.BatchNormalization(momentum=0.8)(c2)
        c2 = layers.Add()([c2, c1])

        # Upsampling
        u1 = deconv2d(c2)
        u2 = deconv2d(u1)
        u3 = deconv2d(u2)

        # Generate high resolution output
        gen_hr = layers.Conv2D(self.channels, kernel_size=9, strides=1, padding='same', activation='tanh')(u3)
        gen_hr = gen_hr * 0.5 + 0.5

        return Model(img_lr, gen_hr)

    def train(self, epochs, batch_size):

        # Load the dataset
        train_generator = image_generator(batch_size, train_dir)

        self.model.fit_generator(train_generator,
                    steps_per_epoch=config.steps_per_epoch,
                    epochs=config.num_epochs, callbacks=[ImageLogger_save(), WandbCallback()],
                    validation_steps=config.val_steps_per_epoch,
                    validation_data=val_generator)

ffn = FFN()
ffn.train(epochs=config.num_epochs, batch_size=config.batch_size)

